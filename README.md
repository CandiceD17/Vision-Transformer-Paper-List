# Vision Transformer Paper List
A thorough paper list for the most recent progress in vision transformer.

## Contents

- [Performance](https://github.com/CandiceD17/vision-transformer-paper-list#performance)
- Model
  - [Transformer in General](https://github.com/CandiceD17/vision-transformer-paper-list#transformer-in-general)
  - [Survey](https://github.com/CandiceD17/vision-transformer-paper-list#survey)
  - [Self-Attention Network](https://github.com/CandiceD17/vision-transformer-paper-list#self-attention-network)
  - [Vision Transformer](https://github.com/CandiceD17/vision-transformer-paper-list#vision-transformer)
- [Blog](https://github.com/CandiceD17/vision-transformer-paper-list#%E7%9F%A5%E4%B9%8E%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3)

## Performance

|      |      |      |
| ---- | ---- | ---- |
|      |      |      |
|      |      |      |
|      |      |      |



## Model

### Transformer in General:

- **[Transformer]** Attention is All You Need | **[NeurIPS 2017]** | [`[PDF]`](https://arxiv.org/pdf/1706.03762v5.pdf) [`[official code - tensorflow]`](https://github.com/tensorflow/models/tree/master/official/nlp/transformer) [`[paper-with-code]`](https://paperswithcode.com/paper/attention-is-all-you-need)

### Survey:

- **[Survey]** Transformers in Vision: A Survey | **[Archive 2021]** | [`[PDF]`](https://arxiv.org/pdf/2101.01169.pdf) 
- **[Survey]** A Survey on Transformer | **[Archive 2021]** | [`[PDF]`](https://arxiv.org/abs/2012.12556.pdf) 
- **[Survey]** Efficient Transformers: A Survey | **[Archive 2020]** | [`[PDF]`](https://arxiv.org/pdf/2009.06732.pdf) 

### Self-Attention Network:

- **[LR-Net]** Local Relation Networks for Image Recognition | **[ICCV 2019]** | [`[PDF]`](https://arxiv.org/pdf/1904.11491.pdf) [`[unofficial code - torch]`](https://github.com/gan3sh500/local-relational-nets) [`[paper-with-code]`](https://paperswithcode.com/paper/190411491#code)

### Vision Transformer:

- **[ViT]** Local Relation Networks for Image Recognition | **[ICLR 2021]** | [`[PDF]`](https://arxiv.org/pdf/1904.11491.pdf) [`[official code - jax]`](https://github.com/google-research/vision_transformer) [`[paper-with-code]`](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1)
- **[T2T-ViT]** Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet | **[Archive 2021]** | [`[PDF]`](https://arxiv.org/pdf/1904.11491.pdf) [`[official code - torch]`](https://github.com/yitu-opensource/T2T-ViT) [`[paper-with-code]`](https://paperswithcode.com/paper/tokens-to-token-vit-training-vision)



## 知乎原理详解：

- 解释Transformer Attention机制 (Attention is All You Need)，详细清晰，原理和训练场景图文并茂 [链接](https://zhuanlan.zhihu.com/p/48508221)
- Transformer原理+两个视觉transformer(ViT & DETR) 的机制和重要源码解读，源码备注很详细 [链接](https://zhuanlan.zhihu.com/p/308301901)